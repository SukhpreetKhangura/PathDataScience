{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tuttorial, we are going to learn how to import data from different files such as Flat files, e..g, csv, txts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\JourneyDataScience\\ImportingDataBasics\\moby_dick.txt\n"
     ]
    }
   ],
   "source": [
    "filename = 'D:\\JourneyDataScience\\ImportingDataBasics\\moby_dick.txt' # Text File to be opened\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "\n",
      "\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\n",
      "\n",
      "\n",
      "\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\n",
      "\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\n",
      "\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\n",
      "\n",
      "known nations of the world.  He loved to dust his old grammars; it\n",
      "\n",
      "somehow mildly reminded him of his mortality.\n",
      "\n",
      "\n",
      "\n",
      "\"While you take in hand to school others, and to teach them by what\n",
      "\n",
      "name a whale-fish is to be called in our tongue leaving out, through\n",
      "\n",
      "ignorance, the letter H, which almost alone maketh the signification\n",
      "\n",
      "of the word, you deliver that which is not true.\" --HACKLUYT\n"
     ]
    }
   ],
   "source": [
    "file=open(filename, mode ='r') # to  open and read the file\n",
    "\n",
    "for line in file:\n",
    "    print(line)\n",
    "\n",
    "file.close() # to close the opened file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using context manager with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ETYMOLOGY.\n",
      "\n",
      "\n",
      "\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\n",
      "\n",
      "\n",
      "\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\n",
      "\n",
      "now.  He was ever dusting his old lexicons and grammars, with a queer\n",
      "\n",
      "handkerchief, mockingly embellished with all the gay flags of all the\n",
      "\n",
      "known nations of the world.  He loved to dust his old grammars; it\n",
      "\n",
      "somehow mildly reminded him of his mortality.\n",
      "\n",
      "\n",
      "\n",
      "\"While you take in hand to school others, and to teach them by what\n",
      "\n",
      "name a whale-fish is to be called in our tongue leaving out, through\n",
      "\n",
      "ignorance, the letter H, which almost alone maketh the signification\n",
      "\n",
      "of the word, you deliver that which is not true.\" --HACKLUYT\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing flat files using NumPy : standard for storing numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\JourneyDataScience\\ImportingDataBasics\\train_labels_MNIST.csv\n"
     ]
    }
   ],
   "source": [
    "filename_csv='D:\\JourneyDataScience\\ImportingDataBasics\\\\train_labels_MNIST.csv'# \n",
    "\n",
    "''' here the name of the files starts with 't', '\\t'rain is considered as tab, there 'note'\\\\ '''\n",
    "print(filename_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "0.0\n",
      "4.0\n",
      "1.0\n",
      "9.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "4.0\n",
      "3.0\n",
      "5.0\n",
      "3.0\n",
      "6.0\n",
      "1.0\n",
      "7.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "file_csv_np= np.loadtxt(filename_csv, delimiter= ',', skiprows=0) # since we don't have headers to be skipped\n",
    "for line in file_csv_np:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other functions such np.recfromcsv() np.genfrontxt(): to handle data of different types in a structured array. \n",
    "\n",
    "Moving towards pandas now inorder to have two dimensional labelled structures that allow statistical analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_csv_df = pd.read_csv('D:\\JourneyDataScience\\ImportingDataBasics\\\\titanic.csv')\n",
    "file_csv_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to import files from other file types: 1. Pickled 2. SAAS 3. Sata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickled Files: Lets create a dictionary and store it as pickled file first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "edu_dict = { \"0\": \"B.tech\", \"1\": \"Mtech\", \"2\" :\"MBA\", \"3\": \"M.Sc.\", \"5\":\"PhD\"}\n",
    "pickle.dump( edu_dict, open( \"D:\\JourneyDataScience\\ImportingDataBasics\\edu.pkl\", \"wb\" ) ) # wb write binary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'B.tech', '1': 'Mtech', '2': 'MBA', '3': 'M.Sc.', '5': 'PhD'}\n",
      "{'0': 'B.tech', '1': 'Mtech', '2': 'MBA', '3': 'M.Sc.', '5': 'PhD'}\n"
     ]
    }
   ],
   "source": [
    "file_pkl = pickle.load( open( \"D:\\JourneyDataScience\\ImportingDataBasics\\edu.pkl\", \"rb\" ) ) # read binary\n",
    "print(file_pkl)\n",
    "\n",
    "\n",
    "# Second Method using with context manager\n",
    "with open(\"D:\\JourneyDataScience\\ImportingDataBasics\\edu.pkl\", 'rb') as file:\n",
    "    file_pckl=pickle.load(file)\n",
    "print(file_pckl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving towards xlsx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\JourneyDataScience\\ImportingDataBasics\\urbanpop.xlsx\n",
      "['1960-1966', '1967-1974', '1975-2011']\n",
      "          country       1960          1961          1962          1963  \\\n",
      "0     Afghanistan   769308.0  8.149230e+05  8.585217e+05  9.039139e+05   \n",
      "1         Albania   494443.0  5.118028e+05  5.294389e+05  5.473767e+05   \n",
      "2         Algeria  3293999.0  3.515148e+06  3.739963e+06  3.973289e+06   \n",
      "3  American Samoa        NaN  1.366030e+04  1.416580e+04  1.475893e+04   \n",
      "4         Andorra        NaN  8.723921e+03  9.700346e+03  1.074838e+04   \n",
      "\n",
      "           1964          1965          1966  \n",
      "0  9.512259e+05  1.000582e+06  1.058743e+06  \n",
      "1  5.655718e+05  5.839829e+05  6.025122e+05  \n",
      "2  4.220987e+06  4.488176e+06  4.649105e+06  \n",
      "3  1.539642e+04  1.604482e+04  1.669311e+04  \n",
      "4  1.186586e+04  1.305275e+04  1.421681e+04  \n",
      "          country          1967          1968          1969          1970  \\\n",
      "0     Afghanistan  1.119067e+06  1.182159e+06  1.248901e+06  1.319849e+06   \n",
      "1         Albania  6.211798e+05  6.399645e+05  6.588531e+05  6.778391e+05   \n",
      "2         Algeria  4.826104e+06  5.017299e+06  5.219332e+06  5.429743e+06   \n",
      "3  American Samoa  1.734866e+04  1.799551e+04  1.861868e+04  1.920639e+04   \n",
      "4         Andorra  1.543962e+04  1.672699e+04  1.808832e+04  1.952896e+04   \n",
      "\n",
      "           1971          1972          1973          1974  \n",
      "0  1.409001e+06  1.502402e+06  1.598835e+06  1.696445e+06  \n",
      "1  6.989322e+05  7.202066e+05  7.416810e+05  7.633855e+05  \n",
      "2  5.619042e+06  5.815734e+06  6.020647e+06  6.235114e+06  \n",
      "3  1.975202e+04  2.026267e+04  2.074197e+04  2.119438e+04  \n",
      "4  2.092873e+04  2.240584e+04  2.393705e+04  2.548198e+04  \n"
     ]
    }
   ],
   "source": [
    "file_xlsx = \"D:\\JourneyDataScience\\ImportingDataBasics\\\\urbanpop.xlsx\"\n",
    "print(file_xlsx)\n",
    "data_xlsx=pd.ExcelFile(file_xlsx)\n",
    "print(data_xlsx.sheet_names) # to load the sheet names\n",
    "\n",
    "# Convert the loaded sheet into dataframes, either by parsing sheet names or by their index\n",
    "df1= data_xlsx.parse('1960-1966')\n",
    "print(df1.head())\n",
    "df2=data_xlsx.parse(1)\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving towards SAS: Statistical Analysis System\n",
    "    Stata: \"Statistics\" + \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DCSKINPRODUCT.sas7bdat] header length 65536 != 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAS7BDAT file: DCSKINPRODUCT.sas7bdat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductKey</th>\n",
       "      <th>DistributionCenter</th>\n",
       "      <th>DATE</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Javier's Deep Cleansing Hair Shampoo 2.718 dl</td>\n",
       "      <td>Cary</td>\n",
       "      <td>19399.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Javier's Deep Cleansing Hair Shampoo 2.718 dl</td>\n",
       "      <td>Cary</td>\n",
       "      <td>19406.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Javier's Deep Cleansing Hair Shampoo 2.718 dl</td>\n",
       "      <td>Cary</td>\n",
       "      <td>19413.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Javier's Deep Cleansing Hair Shampoo 2.718 dl</td>\n",
       "      <td>Cary</td>\n",
       "      <td>19420.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Javier's Deep Cleansing Hair Shampoo 2.718 dl</td>\n",
       "      <td>Cary</td>\n",
       "      <td>19427.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ProductKey DistributionCenter     DATE  \\\n",
       "0  Javier's Deep Cleansing Hair Shampoo 2.718 dl               Cary  19399.0   \n",
       "1  Javier's Deep Cleansing Hair Shampoo 2.718 dl               Cary  19406.0   \n",
       "2  Javier's Deep Cleansing Hair Shampoo 2.718 dl               Cary  19413.0   \n",
       "3  Javier's Deep Cleansing Hair Shampoo 2.718 dl               Cary  19420.0   \n",
       "4  Javier's Deep Cleansing Hair Shampoo 2.718 dl               Cary  19427.0   \n",
       "\n",
       "   Discount  Revenue  \n",
       "0       0.0      0.0  \n",
       "1       0.0      0.0  \n",
       "2       0.0      0.0  \n",
       "3       0.0      0.0  \n",
       "4       0.0      0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sas7bdat import SAS7BDAT\n",
    "with SAS7BDAT (\"D:\\JourneyDataScience\\ImportingDataBasics\\\\DCSKINPRODUCT.sas7bdat\") as file:\n",
    "    print(file)\n",
    "    df_sas =file.to_data_frame()\n",
    "df_sas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stata: \"Statistics\" + \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year      y      w       r      l      k\n",
      "0  1948  1.214  0.243  0.1454  1.415  0.612\n",
      "1  1949  1.354  0.260  0.2181  1.384  0.559\n",
      "2  1950  1.569  0.278  0.3157  1.388  0.573\n",
      "3  1951  1.948  0.297  0.3940  1.550  0.564\n",
      "4  1952  2.265  0.310  0.3559  1.802  0.574\n"
     ]
    }
   ],
   "source": [
    "file_dta ='D:\\JourneyDataScience\\ImportingDataBasics\\\\airline.dta'\n",
    "data_dta=pd.read_stata(file_dta)\n",
    "print(data_dta.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
